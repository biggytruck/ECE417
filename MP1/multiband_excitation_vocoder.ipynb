{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiband Excitation Vocoder\n",
    "\n",
    "This project implements the Multiband Excitation Vocoder algorithm of Griffin and Lim, 1988, to synthesize speech signals comprised of both voiced and unvoiced signals. In the context of speech processing, voiced signals refer to sounds generated by vibration of vocal cords, while unvoiced signals, by contrast, don't involve the use of vocal cords. Making use of the unique characteristics of both signals, the Multiband Excitation Vocoder algorithm analyzes the amplitude and pitch in each STFT frame and synthesizes the signals. Since the algorithm preserves most signal information in synthesis, it is useful in speech coding and signal transmission. More details can be found from the original paper:\n",
    "\n",
    "https://ieeexplore.ieee.org/document/1651, D.W. Griffin and J.S. Lim, 1988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.io.wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Analysis\n",
    "\n",
    "To analyze the signal, we first apply STFT to extract signal frames with overlaps. For a given speech frame $s$ and a window $w$, we compute the autocorrelation of $w^{2}[n]s[n]$:\n",
    "\n",
    "$\\phi(m) = \\sum_{n = -\\infty}^{\\infty} w^{2}[n]s[n]w^{2}[n-m]s[n-m]$\n",
    "\n",
    "which estimates how similar a frame is with its own shifted version. We can thus estimate a rough integer for the pitch:\n",
    "\n",
    "$P_{0} = \\underset{P}{\\operatorname{argmax}} \\phi(P) = \\underset{P}{\\operatorname{argmax}} P\\sum_{k = -\\infty}^{\\infty} \\phi(kP)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract frames\n",
    "# samples_per_frame and samples_per_skip in seconds\n",
    "\n",
    "def extract_frame(data, rate, samples_per_frame, samples_per_skip, discard_last = True):\n",
    "    data_length = data.shape[0]\n",
    "    frames = []\n",
    "    current_sample = 0\n",
    "    while(current_sample <= data_length - samples_per_frame):\n",
    "        current_frame = data[current_sample : current_sample + samples_per_frame]\n",
    "        current_sample = current_sample + samples_per_skip\n",
    "        frames.append(current_frame)\n",
    "    \n",
    "    if(not discard_last):\n",
    "        if current_sample < data_length:    \n",
    "            last_frame = data[current_sample:]\n",
    "            frames.append(last_frame)\n",
    "    \n",
    "    frames = np.array(frames)\n",
    "    return frames\n",
    "\n",
    "# apply fft to windowed frames\n",
    "def fft_frame(frames, fft_length):\n",
    "    frame_count = frames.shape[0]\n",
    "    fft_frames = []\n",
    "    for f in range(frame_count):\n",
    "        current_frame = frames[f] * np.hamming(frames[f].shape[0])\n",
    "        fft_frames.append(np.fft.fft(current_frame, fft_length))\n",
    "    fft_frames = np.array(fft_frames)\n",
    "    return fft_frames\n",
    "\n",
    "# estimate rough pitch integers for a given pitch interval\n",
    "def estimate_pitch(frames,samples_per_frame,periods):\n",
    "    pitch_estimates = []\n",
    "    for f in frames:\n",
    "        window = np.hamming(samples_per_frame)\n",
    "        scaled_frame = np.multiply(np.multiply(window,window),f)\n",
    "        autocorr = np.correlate(scaled_frame,scaled_frame,'full')\n",
    "        autocorr_index = np.arange(-samples_per_frame + 1, samples_per_frame)\n",
    "        theta_p = []\n",
    "        for p in periods:\n",
    "            min_k = int((-samples_per_frame + 1)/p)\n",
    "            max_k = int(samples_per_frame/p)\n",
    "            k = np.arange(min_k, max_k)\n",
    "            theta_p.append(p * np.sum(autocorr[k * p + samples_per_frame-1]))\n",
    "        theta_p = np.array(theta_p)\n",
    "        max_theta_p = np.argmax(theta_p)\n",
    "        rough_estimate = periods[max_theta_p]\n",
    "        pitch_estimates.append(rough_estimate)\n",
    "    pitch_estimates = np.array(pitch_estimates)\n",
    "    return pitch_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to find the amplitude of each frame to reconstruct the signal. For a given rough pitch estimate $P_{0}$:\n",
    "\n",
    "$w_{0} = \\frac{2\\pi}{P_{0}}$\n",
    "\n",
    "which is the fundamental frequency. We determine the band frequencies by:\n",
    "\n",
    "$[a_{m}, b_{m}] = [(m-\\frac{1}{2})w_{0}, (m+\\frac{1}{2})w_{0}], m = 1,2,3,...,P_{0}-1$\n",
    "\n",
    "We can now estimate the amplitude by:\n",
    "\n",
    "$A_{m} = \\frac{\\int_{a_{m}}^{b_{m}} S_{w}(w)E^{*}(w)}{\\int_{a_{m}}^{b_{m}} |E(w)|^{2}}$\n",
    "\n",
    "$\\bullet$ Assume that the $m$-th band is voiced: take $E_{w}(w)$ as the Fourier transform of the window centered around $mw_{0}$\n",
    "\n",
    "$\\bullet$ Assume that the m-th band is unvoiced: take $E_{w}(w) = 1$, $w ∈ [am, bm]$\n",
    "\n",
    "$\\bullet$ For each case, compute the error $\\epsilon_{m} = \\frac{1}{2\\pi} \\int_{a_{m}}^{b_{m}} |S_{w}(w)-A_{m}E_{w}(w)|^{2}$\n",
    "\n",
    "$\\bullet$ If $\\epsilon_{m, voiced} < \\epsilon_{m, unvoiced}$, the band is voiced. Otherwise it is unvoiced.\n",
    "\n",
    "$\\bullet$ Pick the corresponding $A_{m}$\n",
    "\n",
    "Finally, we compute the total error by:\n",
    "\n",
    "$\\epsilon(P_{0}) = \\sum_{m = 0}^{m = P_{0}-1} \\epsilon_{m}(P_{0})$\n",
    "\n",
    "We can be done here for the analysis part. However, the integer estimate of the frame pitch is rough, so we want to improve the performance by refining the pitch. For each rough pitch estimate $P_{0}$, we generate more pitch candidates by finding $P \\in [P_{0} − 2, P_{0} − 1.8, P_{0} − 1.6, . . . , P_{0} + 1.8, P_{0} + 2]$ and repeat the above procedures to determine the refined pitches. The final estimate is determined by:\n",
    "\n",
    "$P = \\underset{P}{\\operatorname{argmin} \\epsilon(P)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine pitches based on rough pitch integers and estimate amplitude\n",
    "def estimate_amplitude(fft_frames,samples_per_frame,pitch_estimates,fft_length):\n",
    "    Ams = []\n",
    "    choices = []\n",
    "    ems = []\n",
    "    error_sum = []\n",
    "    refined_pitch_estimates = []\n",
    "    for i in range(len(pitch_estimates)):\n",
    "        fft_frame = fft_frames[i]\n",
    "        rough_pitch = pitch_estimates[i]\n",
    "        refined_pitches = np.arange(rough_pitch-2, rough_pitch+2+0.2, 0.2)\n",
    "        \n",
    "        min_error = np.inf      \n",
    "        best_Am = []\n",
    "        best_choices = []\n",
    "        best_ems = []\n",
    "        best_pitch = -1\n",
    "        for refined_pitch in refined_pitches:\n",
    "            w_0 = 2 * np.pi / refined_pitch\n",
    "            refined_Ams = []\n",
    "            refined_choices = []\n",
    "            refined_ems = []\n",
    "            \n",
    "            for m in range(1,int(refined_pitch)):\n",
    "                lower_freq = w_0 * (m-1/2)\n",
    "                upper_freq = w_0 * (m+1/2)\n",
    "                \n",
    "                freq_rate = (2*np.pi/fft_length)\n",
    "                lower_index = int(lower_freq / freq_rate) + 1\n",
    "                upper_index = min(int(upper_freq / freq_rate), fft_length - 1)\n",
    "                                  \n",
    "                Sw = fft_frame[lower_index:upper_index+1]\n",
    "                norm = np.sum(Sw *np.conj(Sw))\n",
    "                fft_window = np.fft.fft(np.hamming(samples_per_frame),fft_length)\n",
    "                                  \n",
    "                Ew_voiced = np.fft.fftshift(fft_window[lower_index:upper_index+1])\n",
    "                numer_Am_voiced = Sw * np.conj(Ew_voiced)\n",
    "                denom_Am_voiced = Ew_voiced * np.conj(Ew_voiced)\n",
    "                numer_integrand = (np.sum(numer_Am_voiced) - 1/2*(numer_Am_voiced[0]+numer_Am_voiced[-1]))\n",
    "                denom_integrand = (np.sum(denom_Am_voiced) - 1/2*(denom_Am_voiced[0]+denom_Am_voiced[-1]))\n",
    "                Am_voiced = numer_integrand / denom_integrand\n",
    "                em_integrand = (Sw - Am_voiced * Ew_voiced) * np.conj((Sw - Am_voiced * Ew_voiced))\n",
    "                em_voiced =  (np.sum(em_integrand) - 1/2*(em_integrand[0]+em_integrand[-1]))\n",
    "                \n",
    "                Ew_unvoiced = np.ones(len(Sw))\n",
    "                numer_Am_unvoiced = Sw * np.conj(Ew_unvoiced)\n",
    "                denom_Am_unvoiced = Ew_unvoiced * np.conj(Ew_unvoiced)\n",
    "                numer_integrand = (np.sum(numer_Am_unvoiced) - 1/2*(numer_Am_unvoiced[0]+numer_Am_unvoiced[-1]))\n",
    "                denom_integrand = (np.sum(denom_Am_unvoiced) - 1/2*(denom_Am_unvoiced[0]+denom_Am_unvoiced[-1]))\n",
    "                Am_unvoiced = numer_integrand / denom_integrand\n",
    "                em_integrand_un = (Sw - Am_unvoiced * Ew_unvoiced) * np.conj((Sw - Am_unvoiced * Ew_unvoiced))\n",
    "                em_unvoiced = (np.sum(em_integrand_un) - 1/2*(em_integrand_un[0]+em_integrand_un[-1]))\n",
    "                \n",
    "                if (em_voiced > em_unvoiced):\n",
    "                    refined_Ams.append(Am_unvoiced)\n",
    "                    refined_choices.append(0)\n",
    "                    refined_ems.append(np.absolute(em_unvoiced))           \n",
    "                                  \n",
    "                else:\n",
    "                    refined_Ams.append(Am_voiced)\n",
    "                    refined_choices.append(1)\n",
    "                    refined_ems.append(np.absolute(em_voiced))\n",
    "                    \n",
    "            current_error_sum = np.sum(refined_ems)\n",
    "            if(current_error_sum < min_error):\n",
    "                min_error = current_error_sum\n",
    "                best_Am = refined_Ams\n",
    "                best_choices = refined_choices\n",
    "                best_em = refined_ems\n",
    "                best_pitch = refined_pitch\n",
    "            \n",
    "        Ams.append(best_Am)\n",
    "        choices.append(best_choices)\n",
    "        ems.append(best_em)\n",
    "        error_sum.append(min_error)\n",
    "        refined_pitch_estimates.append(best_pitch)\n",
    "        \n",
    "    Ams = np.array(Ams)\n",
    "    choices = np.array(choices)\n",
    "    ems = np.array(ems)\n",
    "    refined_pitch_estimates = np.array(refined_pitch_estimates)\n",
    "                                  \n",
    "    return Ams, choices, ems, refined_pitch_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Synthesis\n",
    "\n",
    "Speech synthesis can be decomposed into the reconstruction of voiced and unvoiced signals, respectively. For voiced signals, each sample between $[fK, (f+1)K]$ can be reconstructed by:\n",
    "\n",
    "$s_{v}(n) = \\sum_{m} = A_m[n]cos(\\theta_{m}[n])$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\theta_{m}[n] = \\theta_{m}[n-1] + mw_{0}$\n",
    "\n",
    "$w_{0}[n] = (f+1 - \\frac{n}{K}) \\frac{2\\pi}{P_{f}} + (\\frac{n}{K}-f) \\frac{2\\pi}{P_{f+1}}$\n",
    "\n",
    "$A_{m}[n] = (f+1 - \\frac{n}{K}) A_{m,f} + (\\frac{n}{K}-f) A_{m,f+1}$\n",
    "\n",
    "Note:\n",
    "\n",
    "$\\bullet$ Assume $A_{m,f} = 0$ for unvoiced bands\n",
    "\n",
    "$\\bullet$ Due to differences between pitch estimates between consecutive frames, the number of bands can change between frames. For the nonexistent bands, assume that $A_{m} = 0$\n",
    "\n",
    "$\\bullet$ For the first frame $(f = 0)$, assume that $\\theta_{m}[-1] = 0$\n",
    "\n",
    "$\\bullet$ For the last frame, take $w_{0} = \\frac{2\\pi}{P_{f}}, A_{m}[n] = A_{m,f}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate voiced signal\n",
    "def voiced_signal(Ams, choices, refined_pitch_estimates, samples_per_skip, signal_length):\n",
    "    voiced_signal = np.zeros(signal_length,dtype = 'complex128')\n",
    "    \n",
    "    prev_theta = []\n",
    "    for f in range(len(refined_pitch_estimates)):\n",
    "        start_n = f*samples_per_skip\n",
    "        end_n = (f+1)*samples_per_skip\n",
    "        for n in range(start_n,end_n):\n",
    "            current_theta = []\n",
    "            if f == len(refined_pitch_estimates) - 1:\n",
    "                max_m = int(refined_pitch_estimates[f]) - 1\n",
    "            else:\n",
    "                max_m = max(int(refined_pitch_estimates[f]), int(refined_pitch_estimates[f+1]))-1\n",
    "\n",
    "            for m in range(max_m):\n",
    "                \n",
    "                if f == len(refined_pitch_estimates) - 1:\n",
    "                    w_0 = 2* np.pi /refined_pitch_estimates[f]\n",
    "                else:\n",
    "                    w_f = 2* np.pi /refined_pitch_estimates[f]\n",
    "                    w_f1 = 2* np.pi /refined_pitch_estimates[f+1]\n",
    "                    w_0 = (f+1- n/samples_per_skip) * w_f + (n/samples_per_skip - f) * w_f1\n",
    "                    \n",
    "                if n == 0:\n",
    "                    theta_m = (m+1) * w_0\n",
    "                elif m >= len(prev_theta):\n",
    "                    theta_m = (m+1) * w_0\n",
    "                else:\n",
    "                    theta_m = prev_theta[m] + (m+1) * w_0\n",
    "                current_theta.append(theta_m)\n",
    "                \n",
    "                if f == len(refined_pitch_estimates) - 1:\n",
    "                    A_m_n = Ams[f][m]\n",
    "\n",
    "                else:\n",
    "                    if m >= int(refined_pitch_estimates[f])-1:\n",
    "                        Am_f = 0\n",
    "                    elif choices[f][m] == 0:\n",
    "                        Am_f = 0\n",
    "                    else:\n",
    "                        Am_f = Ams[f][m]\n",
    "                    if m >= int(refined_pitch_estimates[f+1])-1:\n",
    "                        Am_f1 = 0\n",
    "                    elif choices[f+1][m] == 0:\n",
    "                        Am_f1 = 0\n",
    "                    else:\n",
    "                        Am_f1 = Ams[f+1][m]\n",
    "                    A_m_n = (f+1- n/samples_per_skip) * Am_f + (n/samples_per_skip - f) * Am_f1\n",
    "                    \n",
    "                voiced_signal[n] += A_m_n * np.cos(theta_m)\n",
    "                \n",
    "            prev_theta = current_theta\n",
    "            \n",
    "    return voiced_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unvoiced signals, each sample between $[fK, (f+1)K]$ can be reconstructed by:\n",
    "\n",
    "$s_u[n] = (f+1 - \\frac{n}{K}) u_{f}[n-fK] + (\\frac{n}{K}-f) u_{f+1}[n-(f+1)K]$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\bullet$ $u_f[n]$ is the inverse FFT of the unvoiced sample $U_{f}[k]$\n",
    "\n",
    "$\\bullet$ $U_f[k] = $ \\begin{cases}\n",
    "0, \\space\\space\\space\\frac{2\\pi K}{N_{fft}}\\space is\\space voiced,\n",
    "\\\\ \\mathcal{N}(0, 0.05\\sigma_{m}^{2}) + j\\mathcal{N}(0, 0.05\\sigma_{m}^{2}),\\space else\n",
    "\\end{cases}\n",
    "\n",
    "$\\bullet$ $\\sigma_{m} = \\frac{1}{b_{m}}{a_{m}} \\int_{a_{m}}^{b_{m}} |S_{w}(w)|^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate unvoiced signal\n",
    "def unvoiced_signal(choices, pitch_estimates, samples_per_frame, samples_per_skip, signal_length, fft_frames, fft_length):\n",
    "    unvoiced_signal = np.zeros(signal_length,dtype ='complex128')\n",
    "    \n",
    "    # reconstruct unvoiced signals between [fK, (f+1)K]\n",
    "    unvoiced_frames = []    \n",
    "    frame_num = len(fft_frames)\n",
    "    for f in range(frame_num):\n",
    "        unvoiced_bands_fft = np.zeros(fft_length, dtype ='complex128') # fft of unvoiced bands in current frame\n",
    "        fft_frame = fft_frames[f]\n",
    "        pitch = pitch_estimates[f]\n",
    "        w_0 = 2 * np.pi / pitch\n",
    "        \n",
    "        for m in range(0,int(pitch)-1):\n",
    "            \n",
    "            if choices[f][m] == 0:\n",
    "                \n",
    "                center_freq = w_0 * m\n",
    "                lower_freq = w_0 * (m-1/2)\n",
    "                upper_freq = w_0 * (m+1/2)\n",
    "\n",
    "                freq_rate = fft_length / (2 * np.pi)\n",
    "                lower_index = int(lower_freq * freq_rate) + 1\n",
    "                upper_index = min(int(upper_freq * freq_rate),fft_length - 1)\n",
    "                \n",
    "                Sw = fft_frame[lower_index:upper_index+1]\n",
    "                var = 1/(upper_index - lower_index) * np.sum(Sw * np.conj(Sw))\n",
    "                unvoiced_var = 0.5 * np.absolute(var)\n",
    "                \n",
    "                for k in range(lower_index, upper_index + 1):\n",
    "                    unvoiced_bands_fft[k] = complex(np.random.normal(0,np.sqrt(unvoiced_var)),\n",
    "                                                    np.random.normal(0,np.sqrt(unvoiced_var)))\n",
    "        \n",
    "        unvoiced_bands = np.fft.ifft(unvoiced_bands_fft,fft_length) # unvoiced signal in current frame\n",
    "        unvoiced_frames.append(unvoiced_bands[0:samples_per_frame])\n",
    "        \n",
    "    unvoiced_frames = np.array(unvoiced_frames)\n",
    "    for f in range(frame_num):\n",
    "        start_n = f * samples_per_skip\n",
    "        end_n = (f+1) * samples_per_skip\n",
    "        for n in range(start_n,end_n):\n",
    "            if f == frame_num - 1:\n",
    "                unvoiced_signal[n] = unvoiced_frames[f][n-f*samples_per_skip]\n",
    "            else:\n",
    "                curr_unvoiced = (f+1- n/samples_per_skip) * unvoiced_frames[f][n-f*samples_per_skip]\n",
    "                next_unvoiced = (n/samples_per_skip - f) * unvoiced_frames[f+1][n-(f+1)*samples_per_skip]\n",
    "                unvoiced_signal[n] = curr_unvoiced + next_unvoiced\n",
    "                \n",
    "    return unvoiced_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### pre-process data\n",
    "rate, data = scipy.io.wavfile.read('s5.wav')\n",
    "data = data.astype('float32')\n",
    "norm = np.sqrt(np.sum(data*data))\n",
    "data = data/norm\n",
    "samples_per_frame = int(25/1000*rate)\n",
    "samples_per_skip = int(10/1000*rate)\n",
    "signal_length = data.shape[0]\n",
    "fft_length = 1024\n",
    "pitch_candidates = np.arange(20, 91)\n",
    "\n",
    "### speech analysis\n",
    "frames = extract_frame(data, \n",
    "                       rate, \n",
    "                       samples_per_frame, \n",
    "                       samples_per_skip)\n",
    "\n",
    "fft_frames = fft_frame(frames, fft_length)\n",
    "\n",
    "pitch_estimates = estimate_pitch(frames, \n",
    "                                 samples_per_frame, \n",
    "                                 pitch_candidates)\n",
    "\n",
    "Ams, choices, errors, refined_pitch_estimates = estimate_amplitude(fft_frames,\n",
    "                                                           samples_per_frame,\n",
    "                                                           pitch_estimates,\n",
    "                                                           fft_length)\n",
    "\n",
    "### speech synthesis\n",
    "voiced_signal = voiced_signal(Ams, \n",
    "                              choices,\n",
    "                              refined_pitch_estimates,\n",
    "                              samples_per_skip,\n",
    "                              signal_length)\n",
    "\n",
    "unvoiced_signal = unvoiced_signal(choices,\n",
    "                                  refined_pitch_estimates,\n",
    "                                  samples_per_frame,\n",
    "                                  samples_per_skip, \n",
    "                                  signal_length, \n",
    "                                  fft_frames, \n",
    "                                  fft_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.wavfile.write(\"synthesis.wav\",rate,np.real(voiced_signal+unvoiced_signal))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
