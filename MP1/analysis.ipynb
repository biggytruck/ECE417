{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.io.wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts the frames\n",
    "# samples_per_frame and samples_per_skip in seconds\n",
    "\n",
    "def extract_frame(data, rate, samples_per_frame, samples_per_skip, discard_last = True):\n",
    "    data_length = data.shape[0]\n",
    "    frames = []\n",
    "    current_sample = 0\n",
    "    while(current_sample <= data_length - samples_per_frame):\n",
    "        current_frame = data[current_sample : current_sample + samples_per_frame]\n",
    "        current_sample = current_sample + samples_per_skip\n",
    "        frames.append(current_frame)\n",
    "    \n",
    "    if(not discard_last):\n",
    "        if current_sample < data_length:    \n",
    "            last_frame = data[current_sample:]\n",
    "            frames.append(last_frame)\n",
    "    \n",
    "    frames = np.array(frames)\n",
    "    return frames\n",
    "\n",
    "# This function applies fft to windowed frames\n",
    "def fft_frame(frames, fft_length):\n",
    "    frame_count = frames.shape[0]\n",
    "    fft_frames = []\n",
    "    for f in range(frame_count):\n",
    "        current_frame = frames[f] * np.hamming(frames[f].shape[0])\n",
    "        fft_frames.append(np.fft.fft(current_frame, fft_length))\n",
    "    fft_frames = np.array(fft_frames)\n",
    "    return fft_frames\n",
    "\n",
    "# This function estimates rough pitch integers for a given pitch interval\n",
    "def estimate_pitch(frames,samples_per_frame,periods):\n",
    "    pitch_estimates = []\n",
    "    for f in frames:\n",
    "        window = np.hamming(samples_per_frame)\n",
    "        scaled_frame = np.multiply(np.multiply(window,window),f)\n",
    "        autocorr = np.correlate(scaled_frame,scaled_frame,'full')\n",
    "        autocorr_index = np.arange(-samples_per_frame + 1, samples_per_frame)\n",
    "        theta_p = []\n",
    "        for p in periods:\n",
    "            min_k = int((-samples_per_frame + 1)/p)\n",
    "            max_k = int(samples_per_frame/p)\n",
    "            k = np.arange(min_k, max_k)\n",
    "            theta_p.append(p * np.sum(autocorr[k * p + samples_per_frame-1]))\n",
    "        theta_p = np.array(theta_p)\n",
    "        max_theta_p = np.argmax(theta_p)\n",
    "        rough_estimate = periods[max_theta_p]\n",
    "        pitch_estimates.append(rough_estimate)\n",
    "    pitch_estimates = np.array(pitch_estimates)\n",
    "    return pitch_estimates\n",
    "\n",
    "# This function refines pitch period based on rough integer estimate\n",
    "def estimate_amplitude(fft_frames,samples_per_frame,pitch_estimates,fft_length):\n",
    "    Ams = []\n",
    "    choices = []\n",
    "    ems = []\n",
    "    error_sum = []\n",
    "    refined_pitch_estimates = []\n",
    "    for i in range(len(pitch_estimates)):\n",
    "        fft_frame = fft_frames[i]\n",
    "        rough_pitch = pitch_estimates[i]\n",
    "        refined_pitches = np.arange(rough_pitch-2, rough_pitch+2+0.2, 0.2)\n",
    "        \n",
    "        min_error = np.inf      \n",
    "        best_Am = []\n",
    "        best_choices = []\n",
    "        best_ems = []\n",
    "        best_pitch = -1\n",
    "        for refined_pitch in refined_pitches:\n",
    "            w_0 = 2 * np.pi / refined_pitch\n",
    "            refined_Ams = []\n",
    "            refined_choices = []\n",
    "            refined_ems = []\n",
    "            \n",
    "            for m in range(1,int(refined_pitch)):\n",
    "                center_freq = w_0 * m\n",
    "                lower_freq = w_0 * (m-1/2)\n",
    "                upper_freq = w_0 * (m+1/2)\n",
    "                \n",
    "                freq_rate = (2*np.pi/fft_length)\n",
    "                lower_index = int(lower_freq / freq_rate) + 1\n",
    "                upper_index = min(int(upper_freq / freq_rate), fft_length - 1)\n",
    "                middle_index = int(np.around(center_freq / freq_rate))\n",
    "                                  \n",
    "                Sw = fft_frame[lower_index:upper_index+1]\n",
    "                norm = np.sum(Sw *np.conj(Sw))\n",
    "                fft_window = np.fft.fft(np.hamming(samples_per_frame),fft_length)\n",
    "                                  \n",
    "                Ew_voiced = np.append(fft_window[fft_length - (middle_index - lower_index):fft_length],\n",
    "                                      fft_window[0:upper_index - middle_index + 1])\n",
    "                numer_Am_voiced = Sw * np.conj(Ew_voiced)\n",
    "                denom_Am_voiced = Ew_voiced * np.conj(Ew_voiced)\n",
    "                numer_integrand = (np.sum(numer_Am_voiced) - 1/2*(numer_Am_voiced[0]+numer_Am_voiced[-1]))\n",
    "                denom_integrand = (np.sum(denom_Am_voiced) - 1/2*(denom_Am_voiced[0]+denom_Am_voiced[-1]))\n",
    "                Am_voiced = numer_integrand / denom_integrand\n",
    "                em_integrand = (Sw - Am_voiced * Ew_voiced) * np.conj((Sw - Am_voiced * Ew_voiced))\n",
    "                em_voiced =  (np.sum(em_integrand) - 1/2*(em_integrand[0]+em_integrand[-1]))\n",
    "                \n",
    "                Ew_unvoiced = np.ones(len(Sw))\n",
    "                numer_Am_unvoiced = Sw * np.conj(Ew_unvoiced)\n",
    "                denom_Am_unvoiced = Ew_unvoiced * np.conj(Ew_unvoiced)\n",
    "                numer_integrand = (np.sum(numer_Am_unvoiced) - 1/2*(numer_Am_unvoiced[0]+numer_Am_unvoiced[-1]))\n",
    "                denom_integrand = (np.sum(denom_Am_unvoiced) - 1/2*(denom_Am_unvoiced[0]+denom_Am_unvoiced[-1]))\n",
    "                Am_unvoiced = numer_integrand / denom_integrand\n",
    "                em_integrand_un = (Sw - Am_unvoiced * Ew_unvoiced) * np.conj((Sw - Am_unvoiced * Ew_unvoiced))\n",
    "                em_unvoiced = (np.sum(em_integrand_un) - 1/2*(em_integrand_un[0]+em_integrand_un[-1]))\n",
    "                \n",
    "                if (em_voiced > em_unvoiced):\n",
    "                    refined_Ams.append(Am_unvoiced)\n",
    "                    refined_choices.append(0)\n",
    "                    refined_ems.append(np.absolute(em_unvoiced))           \n",
    "                                  \n",
    "                else:\n",
    "                    refined_Ams.append(Am_voiced)\n",
    "                    refined_choices.append(1)\n",
    "                    refined_ems.append(np.absolute(em_voiced))\n",
    "                    \n",
    "            current_error_sum = np.sum(refined_ems)\n",
    "            if(current_error_sum < min_error):\n",
    "                min_error = current_error_sum\n",
    "                best_Am = refined_Ams\n",
    "                best_choices = refined_choices\n",
    "                best_em = refined_ems\n",
    "                best_pitch = refined_pitch\n",
    "            \n",
    "        Ams.append(best_Am)\n",
    "        choices.append(best_choices)\n",
    "        ems.append(best_em)\n",
    "        error_sum.append(min_error)\n",
    "        refined_pitch_estimates.append(best_pitch)\n",
    "        \n",
    "    Ams = np.array(Ams)\n",
    "    choices = np.array(choices)\n",
    "    ems = np.array(ems)\n",
    "    refined_pitch_estimates = np.array(refined_pitch_estimates)\n",
    "                                  \n",
    "    return Ams, choices, ems, refined_pitch_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates voiced signal\n",
    "\n",
    "def voiced_signal(Ams, choices, refined_pitch_estimates, samples_per_skip, signal_length):\n",
    "    voiced_signal = np.zeros(signal_length,dtype = 'complex128')\n",
    "    \n",
    "    prev_theta = []\n",
    "    for f in range(len(refined_pitch_estimates)):\n",
    "        start_n = f*samples_per_skip\n",
    "        end_n = (f+1)*samples_per_skip\n",
    "        for n in range(start_n,end_n):\n",
    "            current_theta = []\n",
    "            if f == len(refined_pitch_estimates) - 1:\n",
    "                max_m = int(refined_pitch_estimates[f]) - 1\n",
    "            else:\n",
    "                max_m = max(int(refined_pitch_estimates[f]), int(refined_pitch_estimates[f+1]))-1\n",
    "\n",
    "            for m in range(max_m):\n",
    "                \n",
    "                if f == len(refined_pitch_estimates) - 1:\n",
    "                    w_0 = 2* np.pi /refined_pitch_estimates[f]\n",
    "                else:\n",
    "                    w_f = 2* np.pi /refined_pitch_estimates[f]\n",
    "                    w_f1 = 2* np.pi /refined_pitch_estimates[f+1]\n",
    "                    w_0 = (f+1- n/samples_per_skip) * w_f + (n/samples_per_skip - f) * w_f1\n",
    "                    \n",
    "                if n == 0:\n",
    "                    theta_m = (m+1) * w_0\n",
    "                elif m >= len(prev_theta):\n",
    "                    theta_m = (m+1) * w_0\n",
    "                else:\n",
    "                    theta_m = prev_theta[m] + (m+1) * w_0\n",
    "                current_theta.append(theta_m)\n",
    "                \n",
    "                if f == len(refined_pitch_estimates) - 1:\n",
    "                    A_m_n = Ams[f][m]\n",
    "\n",
    "                else:\n",
    "                    if m >= int(refined_pitch_estimates[f])-1:\n",
    "                        Am_f = 0\n",
    "                    elif choices[f][m] == 0:\n",
    "                        Am_f = 0\n",
    "                    else:\n",
    "                        Am_f = Ams[f][m]\n",
    "                    if m >= int(refined_pitch_estimates[f+1])-1:\n",
    "                        Am_f1 = 0\n",
    "                    elif choices[f+1][m] == 0:\n",
    "                        Am_f1 = 0\n",
    "                    else:\n",
    "                        Am_f1 = Ams[f+1][m]\n",
    "                    A_m_n = (f+1- n/samples_per_skip) * Am_f + (n/samples_per_skip - f) * Am_f1\n",
    "                    \n",
    "                voiced_signal[n] += A_m_n * np.cos(theta_m)\n",
    "                \n",
    "            prev_theta = current_theta\n",
    "            \n",
    "    return voiced_signal\n",
    "\n",
    "def unvoiced_signal(choices, pitch_estimates, samples_per_frame, samples_per_skip, signal_length, fft_frames, fft_length):\n",
    "    unvoiced_signal = np.zeros(signal_length,dtype ='complex128')\n",
    "    \n",
    "    # reconstruct unvoiced signals between [fK, (f+1)K]\n",
    "    unvoiced_frames = []    \n",
    "    frame_num = len(fft_frames)\n",
    "    for f in range(frame_num):\n",
    "        unvoiced_bands_fft = np.zeros(fft_length, dtype ='complex128') # fft of unvoiced bands in current frame\n",
    "        fft_frame = fft_frames[f]\n",
    "        pitch = pitch_estimates[f]\n",
    "        w_0 = 2 * np.pi / pitch\n",
    "        \n",
    "        for m in range(0,int(pitch)-1):\n",
    "            \n",
    "            if choices[f][m] == 0:\n",
    "                \n",
    "                center_freq = w_0 * m\n",
    "                lower_freq = w_0 * (m-1/2)\n",
    "                upper_freq = w_0 * (m+1/2)\n",
    "\n",
    "                freq_rate = fft_length / (2 * np.pi)\n",
    "                a_m = int(lower_freq * freq_rate) + 1\n",
    "                b_m = min(int(upper_freq * freq_rate),fft_length - 1)\n",
    "                \n",
    "                Sw = fft_frame[a_m:b_m+1]\n",
    "                var = 1/(b_m - a_m) * np.sum(Sw * np.conj(Sw))\n",
    "                unvoiced_var = 0.5 * np.absolute(var)\n",
    "                \n",
    "                for k in range(a_m, b_m + 1):\n",
    "                    unvoiced_bands_fft[k] = complex(np.random.normal(0,np.sqrt(unvoiced_var)),\n",
    "                                                    np.random.normal(0,np.sqrt(unvoiced_var)))\n",
    "        \n",
    "        unvoiced_bands = np.fft.ifft(unvoiced_bands_fft,fft_length) # unvoiced signal in current frame\n",
    "        unvoiced_frames.append(unvoiced_bands[0:samples_per_frame])\n",
    "        \n",
    "    unvoiced_frames = np.array(unvoiced_frames)\n",
    "    for f in range(frame_num):\n",
    "        start_n = f * samples_per_skip\n",
    "        end_n = (f+1) * samples_per_skip\n",
    "        for n in range(start_n,end_n):\n",
    "            if f == frame_num - 1:\n",
    "                unvoiced_signal[n] = unvoiced_frames[f][n-f*samples_per_skip]\n",
    "            else:\n",
    "                curr_unvoiced = (f+1- n/samples_per_skip) * unvoiced_frames[f][n-f*samples_per_skip]\n",
    "                next_unvoiced = (n/samples_per_skip - f) * unvoiced_frames[f+1][n-(f+1)*samples_per_skip]\n",
    "                unvoiced_signal[n] = curr_unvoiced + next_unvoiced\n",
    "                \n",
    "    return unvoiced_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rate, data = scipy.io.wavfile.read('s5.wav')\n",
    "data = data.astype('float32')\n",
    "norm = np.sqrt(np.sum(data*data))\n",
    "data = data/norm\n",
    "samples_per_frame = int(25/1000*rate)\n",
    "samples_per_skip = int(10/1000*rate)\n",
    "signal_length = data.shape[0]\n",
    "fft_length = 1024\n",
    "pitch_candidates = np.arange(20, 91)\n",
    "\n",
    "frames = extract_frame(data, \n",
    "                       rate, \n",
    "                       samples_per_frame, \n",
    "                       samples_per_skip)\n",
    "\n",
    "fft_frames = fft_frame(frames, fft_length)\n",
    "\n",
    "pitch_estimates = estimate_pitch(frames, \n",
    "                                 samples_per_frame, \n",
    "                                 pitch_candidates)\n",
    "\n",
    "Ams, choices, errors, refined_pitch_estimates = estimate_amplitude(fft_frames,\n",
    "                                                           samples_per_frame,\n",
    "                                                           pitch_estimates,\n",
    "                                                           fft_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voiced_signal = voiced_signal(Ams, \n",
    "                              choices,\n",
    "                              refined_pitch_estimates,\n",
    "                              samples_per_skip,\n",
    "                              signal_length)\n",
    "\n",
    "unvoiced_signal = unvoiced_signal(choices,\n",
    "                                  refined_pitch_estimates,\n",
    "                                  samples_per_frame,\n",
    "                                  samples_per_skip, \n",
    "                                  signal_length, \n",
    "                                  fft_frames, \n",
    "                                  fft_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.wavfile.write(\"test.wav\",rate,np.real(voiced_signal+unvoiced_signal))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
